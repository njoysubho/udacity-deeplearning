{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [01:09, 2.44MB/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/cifar/cifar-10-python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nN40TWAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d6bc05128>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    x_norm = list()\n",
    "    \n",
    "    for px in x:\n",
    "        x_norm.append( ( px - x_min ) / ( x_max - x_min ) )\n",
    "    \n",
    "    return np.array(x_norm)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def encode(x):\n",
    "    y = np.zeros(10)\n",
    "    np.put(y, x, 1)\n",
    "    return y\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    one_hot=list()\n",
    "    for i in x:\n",
    "        one_hot.append(encode(i))\n",
    "    return np.array(one_hot)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, \\\n",
    "                image_shape[0], image_shape[1], image_shape[2]], name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight=tf.Variable(tf.random_normal([conv_ksize[0], conv_ksize[1], \\\n",
    "                    x_tensor.get_shape().as_list()[3], conv_num_outputs], stddev=0.1))\n",
    "    bias=tf.Variable(tf.random_normal([conv_num_outputs], stddev=0.1))\n",
    "    x_tensor=tf.nn.conv2d(x_tensor,weight,[1,conv_strides[0],conv_strides[1],1],padding='SAME')\n",
    "    x_tensor=tf.nn.bias_add(x_tensor,bias)\n",
    "    x_tensor=tf.nn.relu(x_tensor)\n",
    "    x_tensor=tf.nn.max_pool(x_tensor,[1,pool_ksize[0],pool_ksize[1],1],\\\n",
    "                           [1,pool_strides[0],pool_strides[1],1],padding='SAME')\n",
    "    return x_tensor \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs,None)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    conv_ksize       = (8, 8)\n",
    "    conv_strides     = (conv_ksize[0]/2, conv_ksize[1]/2) # (4, 4)\n",
    "    conv_num_outputs = conv_ksize[0]*conv_ksize[1]        # 64\n",
    "    \n",
    "    pool_ksize       = (conv_strides[0], conv_strides[1]) # (4, 4)\n",
    "    pool_strides     = (pool_ksize[0]/2, pool_ksize[1]/2) # (2, 2)\n",
    "    \n",
    "    num_outputs      = 10\n",
    "    \n",
    "    conv2d_maxpool_layer1=conv2d_maxpool(x,conv_num_outputs, conv_ksize, conv_strides, \\\n",
    "                                pool_ksize, pool_strides)\n",
    "    conv2d_maxpool_layer2=conv2d_maxpool(conv2d_maxpool_layer1,conv_num_outputs, conv_ksize, conv_strides, \\\n",
    "                                pool_ksize, pool_strides)\n",
    "    conv2d_maxpool_layer3=conv2d_maxpool(conv2d_maxpool_layer2,conv_num_outputs, conv_ksize, conv_strides, \\\n",
    "                                pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    flatten_layer = flatten(conv2d_maxpool_layer3)\n",
    "    flatten_layer = tf.nn.dropout(flatten_layer, keep_prob)\n",
    "    \n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    fully_conn_1=fully_conn(flatten_layer,num_outputs*4)\n",
    "    fully_conn_1=tf.nn.dropout(fully_conn_1,keep_prob)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    output_layer=output(fully_conn_1,num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, \\\n",
    "                                      y: label_batch, \\\n",
    "                                      keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, \\\n",
    "                                        y: label_batch,   \\\n",
    "                                        keep_prob: 1. })\n",
    "    \n",
    "    accur = session.run(accuracy, feed_dict={x: valid_features, \\\n",
    "                                             y: valid_labels,   \\\n",
    "                                             keep_prob: 1.})\n",
    "    \n",
    "    print('Loss: {:.4f}  Validation Accuracy: {:.2f}%'.format(loss, accur*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 1024\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.2761  Validation Accuracy: 12.16%\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.2616  Validation Accuracy: 18.62%\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 2.2259  Validation Accuracy: 20.46%\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 2.1626  Validation Accuracy: 22.08%\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 2.0903  Validation Accuracy: 25.20%\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 2.0110  Validation Accuracy: 27.74%\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.9272  Validation Accuracy: 29.68%\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.8542  Validation Accuracy: 30.92%\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.8019  Validation Accuracy: 34.06%\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.7701  Validation Accuracy: 35.88%\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.7495  Validation Accuracy: 36.42%\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.6790  Validation Accuracy: 38.64%\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.6353  Validation Accuracy: 39.18%\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.5995  Validation Accuracy: 40.54%\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.5644  Validation Accuracy: 41.48%\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.5561  Validation Accuracy: 41.58%\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.5129  Validation Accuracy: 43.50%\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.4740  Validation Accuracy: 43.50%\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.4866  Validation Accuracy: 42.32%\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.4431  Validation Accuracy: 43.92%\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.4564  Validation Accuracy: 43.46%\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.3999  Validation Accuracy: 44.62%\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.4001  Validation Accuracy: 44.36%\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 1.3763  Validation Accuracy: 45.48%\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 1.3568  Validation Accuracy: 45.84%\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 1.3593  Validation Accuracy: 45.42%\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 1.3362  Validation Accuracy: 46.92%\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 1.3293  Validation Accuracy: 46.62%\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 1.3056  Validation Accuracy: 46.62%\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 1.3043  Validation Accuracy: 47.06%\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 1.2880  Validation Accuracy: 47.36%\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 1.2833  Validation Accuracy: 47.14%\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 1.2701  Validation Accuracy: 47.78%\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 1.2436  Validation Accuracy: 48.36%\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 1.2327  Validation Accuracy: 48.24%\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 1.2206  Validation Accuracy: 47.96%\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 1.2092  Validation Accuracy: 47.92%\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 1.1838  Validation Accuracy: 49.04%\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 1.1754  Validation Accuracy: 49.14%\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 1.1693  Validation Accuracy: 48.90%\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 1.1615  Validation Accuracy: 48.44%\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 1.1565  Validation Accuracy: 49.08%\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 1.1325  Validation Accuracy: 49.18%\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 1.1154  Validation Accuracy: 50.24%\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 1.1121  Validation Accuracy: 49.60%\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 1.1066  Validation Accuracy: 49.82%\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 1.0863  Validation Accuracy: 50.20%\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 1.0737  Validation Accuracy: 50.28%\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 1.0820  Validation Accuracy: 50.40%\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 1.0813  Validation Accuracy: 49.70%\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 1.0977  Validation Accuracy: 49.48%\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 1.0822  Validation Accuracy: 49.20%\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 1.0866  Validation Accuracy: 48.52%\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 1.0468  Validation Accuracy: 49.94%\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 1.0548  Validation Accuracy: 50.38%\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 1.0356  Validation Accuracy: 50.42%\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 1.0023  Validation Accuracy: 50.82%\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 1.0087  Validation Accuracy: 50.30%\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.9966  Validation Accuracy: 50.76%\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.9850  Validation Accuracy: 50.94%\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.9764  Validation Accuracy: 50.90%\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.9780  Validation Accuracy: 50.18%\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.9618  Validation Accuracy: 50.52%\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.9436  Validation Accuracy: 50.54%\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.9431  Validation Accuracy: 51.08%\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.9446  Validation Accuracy: 50.36%\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.9516  Validation Accuracy: 50.26%\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.9917  Validation Accuracy: 49.34%\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.9311  Validation Accuracy: 51.04%\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.9387  Validation Accuracy: 50.54%\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.8995  Validation Accuracy: 51.32%\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.8973  Validation Accuracy: 50.96%\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.8808  Validation Accuracy: 51.36%\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.8690  Validation Accuracy: 51.56%\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.8832  Validation Accuracy: 51.76%\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.8781  Validation Accuracy: 51.28%\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.8613  Validation Accuracy: 51.52%\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.8653  Validation Accuracy: 51.36%\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.8626  Validation Accuracy: 51.54%\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.8512  Validation Accuracy: 51.46%\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.8496  Validation Accuracy: 51.54%\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.8812  Validation Accuracy: 51.28%\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.8800  Validation Accuracy: 51.14%\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.8596  Validation Accuracy: 50.96%\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.8471  Validation Accuracy: 50.44%\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.8448  Validation Accuracy: 50.54%\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.8108  Validation Accuracy: 51.28%\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.8078  Validation Accuracy: 51.20%\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.8063  Validation Accuracy: 51.40%\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.8150  Validation Accuracy: 51.22%\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.8007  Validation Accuracy: 51.38%\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.8338  Validation Accuracy: 50.56%\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.8178  Validation Accuracy: 50.30%\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.8162  Validation Accuracy: 50.28%\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.7996  Validation Accuracy: 50.76%\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.8060  Validation Accuracy: 51.14%\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.8044  Validation Accuracy: 51.10%\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.7773  Validation Accuracy: 51.72%\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.7512  Validation Accuracy: 52.00%\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.7366  Validation Accuracy: 51.74%\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss: 0.7387  Validation Accuracy: 51.14%\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss: 0.7290  Validation Accuracy: 51.48%\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss: 0.7213  Validation Accuracy: 51.18%\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss: 0.7447  Validation Accuracy: 50.22%\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss: 0.7202  Validation Accuracy: 51.12%\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss: 0.7369  Validation Accuracy: 51.64%\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss: 0.7670  Validation Accuracy: 51.38%\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss: 0.7206  Validation Accuracy: 51.78%\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss: 0.7476  Validation Accuracy: 50.74%\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss: 0.6961  Validation Accuracy: 52.08%\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss: 0.6860  Validation Accuracy: 51.52%\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss: 0.7218  Validation Accuracy: 50.66%\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss: 0.6908  Validation Accuracy: 51.88%\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss: 0.6740  Validation Accuracy: 51.32%\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss: 0.6884  Validation Accuracy: 50.82%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116, CIFAR-10 Batch 1:  Loss: 0.6819  Validation Accuracy: 51.68%\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss: 0.6645  Validation Accuracy: 51.18%\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss: 0.6842  Validation Accuracy: 51.34%\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss: 0.6800  Validation Accuracy: 51.68%\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss: 0.6919  Validation Accuracy: 51.08%\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss: 0.6724  Validation Accuracy: 51.54%\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss: 0.6639  Validation Accuracy: 50.84%\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss: 0.6688  Validation Accuracy: 51.06%\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss: 0.6903  Validation Accuracy: 50.42%\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss: 0.6669  Validation Accuracy: 50.72%\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss: 0.6479  Validation Accuracy: 51.48%\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss: 0.6381  Validation Accuracy: 51.20%\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss: 0.6393  Validation Accuracy: 51.22%\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss: 0.6304  Validation Accuracy: 51.32%\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss: 0.6202  Validation Accuracy: 50.66%\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss: 0.6192  Validation Accuracy: 50.86%\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss: 0.6323  Validation Accuracy: 50.48%\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss: 0.6455  Validation Accuracy: 50.04%\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss: 0.6061  Validation Accuracy: 51.50%\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss: 0.6205  Validation Accuracy: 51.88%\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss: 0.6207  Validation Accuracy: 51.60%\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss: 0.5913  Validation Accuracy: 50.98%\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss: 0.5928  Validation Accuracy: 50.80%\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss: 0.5889  Validation Accuracy: 51.50%\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss: 0.5754  Validation Accuracy: 51.92%\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss: 0.5637  Validation Accuracy: 51.96%\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss: 0.5586  Validation Accuracy: 51.36%\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss: 0.5873  Validation Accuracy: 50.88%\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss: 0.5538  Validation Accuracy: 51.40%\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss: 0.5550  Validation Accuracy: 51.84%\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss: 0.5583  Validation Accuracy: 51.88%\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss: 0.5360  Validation Accuracy: 51.70%\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss: 0.5551  Validation Accuracy: 51.90%\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss: 0.5331  Validation Accuracy: 51.94%\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss: 0.5551  Validation Accuracy: 51.32%\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss: 0.5667  Validation Accuracy: 51.04%\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss: 0.5715  Validation Accuracy: 50.46%\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss: 0.5431  Validation Accuracy: 51.34%\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss: 0.5803  Validation Accuracy: 51.34%\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss: 0.6544  Validation Accuracy: 50.00%\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss: 0.5705  Validation Accuracy: 50.54%\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss: 0.5614  Validation Accuracy: 52.02%\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss: 0.5299  Validation Accuracy: 51.72%\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss: 0.5099  Validation Accuracy: 51.46%\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss: 0.5406  Validation Accuracy: 51.44%\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss: 0.5416  Validation Accuracy: 51.18%\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss: 0.5285  Validation Accuracy: 50.28%\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss: 0.5107  Validation Accuracy: 50.98%\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss: 0.5144  Validation Accuracy: 50.78%\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss: 0.4902  Validation Accuracy: 51.36%\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss: 0.5246  Validation Accuracy: 51.38%\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss: 0.5463  Validation Accuracy: 50.94%\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss: 0.4977  Validation Accuracy: 51.70%\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss: 0.4850  Validation Accuracy: 51.02%\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss: 0.4735  Validation Accuracy: 51.08%\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss: 0.4645  Validation Accuracy: 51.38%\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss: 0.4676  Validation Accuracy: 51.34%\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss: 0.4751  Validation Accuracy: 50.94%\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss: 0.4630  Validation Accuracy: 51.20%\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss: 0.4470  Validation Accuracy: 51.44%\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss: 0.4567  Validation Accuracy: 51.68%\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss: 0.4438  Validation Accuracy: 51.58%\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss: 0.4511  Validation Accuracy: 51.40%\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss: 0.4919  Validation Accuracy: 51.30%\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss: 0.4921  Validation Accuracy: 50.74%\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss: 0.4831  Validation Accuracy: 50.54%\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss: 0.5218  Validation Accuracy: 49.34%\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss: 0.4567  Validation Accuracy: 50.94%\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss: 0.4514  Validation Accuracy: 50.96%\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss: 0.4454  Validation Accuracy: 50.90%\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss: 0.4328  Validation Accuracy: 51.72%\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss: 0.4272  Validation Accuracy: 51.68%\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss: 0.4305  Validation Accuracy: 51.30%\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss: 0.4502  Validation Accuracy: 50.74%\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss: 0.4832  Validation Accuracy: 49.84%\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss: 0.4268  Validation Accuracy: 51.48%\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss: 0.4317  Validation Accuracy: 50.80%\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss: 0.4209  Validation Accuracy: 50.80%\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss: 0.3969  Validation Accuracy: 51.18%\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss: 0.4048  Validation Accuracy: 51.20%\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss: 0.4057  Validation Accuracy: 50.92%\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss: 0.4188  Validation Accuracy: 50.90%\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss: 0.4193  Validation Accuracy: 51.08%\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss: 0.4062  Validation Accuracy: 51.40%\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss: 0.4031  Validation Accuracy: 50.84%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.2842  Validation Accuracy: 12.24%\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss: 2.2225  Validation Accuracy: 18.12%\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss: 2.1106  Validation Accuracy: 24.62%\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss: 2.0106  Validation Accuracy: 27.00%\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss: 1.9626  Validation Accuracy: 28.34%\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 1.8991  Validation Accuracy: 32.56%\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss: 1.8341  Validation Accuracy: 33.76%\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss: 1.7432  Validation Accuracy: 36.08%\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss: 1.7088  Validation Accuracy: 37.20%\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss: 1.7347  Validation Accuracy: 36.96%\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.6993  Validation Accuracy: 37.84%\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss: 1.6801  Validation Accuracy: 39.94%\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss: 1.5952  Validation Accuracy: 40.48%\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss: 1.5937  Validation Accuracy: 40.50%\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss: 1.6153  Validation Accuracy: 41.40%\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.5774  Validation Accuracy: 42.48%\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss: 1.5778  Validation Accuracy: 43.00%\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss: 1.5094  Validation Accuracy: 43.72%\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss: 1.5070  Validation Accuracy: 43.88%\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss: 1.5268  Validation Accuracy: 43.82%\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.5037  Validation Accuracy: 44.52%\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss: 1.4902  Validation Accuracy: 45.88%\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss: 1.4350  Validation Accuracy: 46.18%\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss: 1.4384  Validation Accuracy: 46.08%\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss: 1.4601  Validation Accuracy: 46.88%\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.4621  Validation Accuracy: 45.36%\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss: 1.4542  Validation Accuracy: 47.04%\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss: 1.3797  Validation Accuracy: 47.96%\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss: 1.3877  Validation Accuracy: 47.34%\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss: 1.4253  Validation Accuracy: 48.08%\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.4103  Validation Accuracy: 48.40%\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss: 1.4017  Validation Accuracy: 49.04%\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss: 1.3624  Validation Accuracy: 48.50%\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss: 1.3755  Validation Accuracy: 47.70%\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss: 1.3789  Validation Accuracy: 49.78%\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.3744  Validation Accuracy: 49.18%\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss: 1.3526  Validation Accuracy: 50.14%\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss: 1.3192  Validation Accuracy: 49.78%\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss: 1.3200  Validation Accuracy: 50.06%\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss: 1.3432  Validation Accuracy: 50.06%\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.3438  Validation Accuracy: 50.12%\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss: 1.3288  Validation Accuracy: 51.10%\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss: 1.2845  Validation Accuracy: 50.92%\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss: 1.3000  Validation Accuracy: 50.54%\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss: 1.3179  Validation Accuracy: 52.16%\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.3204  Validation Accuracy: 51.06%\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss: 1.2950  Validation Accuracy: 51.34%\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss: 1.2611  Validation Accuracy: 52.14%\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss: 1.2804  Validation Accuracy: 51.20%\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss: 1.3029  Validation Accuracy: 52.16%\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.2818  Validation Accuracy: 52.70%\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss: 1.2652  Validation Accuracy: 52.52%\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss: 1.2342  Validation Accuracy: 52.76%\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss: 1.2345  Validation Accuracy: 53.56%\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss: 1.2698  Validation Accuracy: 53.04%\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.2476  Validation Accuracy: 53.82%\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss: 1.2418  Validation Accuracy: 53.70%\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss: 1.2137  Validation Accuracy: 53.34%\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss: 1.2139  Validation Accuracy: 54.30%\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss: 1.2503  Validation Accuracy: 53.94%\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.2362  Validation Accuracy: 53.12%\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss: 1.2352  Validation Accuracy: 53.78%\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss: 1.2073  Validation Accuracy: 53.38%\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss: 1.1877  Validation Accuracy: 54.60%\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss: 1.2230  Validation Accuracy: 55.00%\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.2124  Validation Accuracy: 53.88%\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss: 1.2262  Validation Accuracy: 53.98%\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss: 1.1781  Validation Accuracy: 54.82%\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss: 1.1747  Validation Accuracy: 54.26%\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss: 1.1985  Validation Accuracy: 55.20%\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.1845  Validation Accuracy: 54.54%\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss: 1.2364  Validation Accuracy: 53.14%\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss: 1.1720  Validation Accuracy: 54.86%\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss: 1.1575  Validation Accuracy: 54.50%\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss: 1.2047  Validation Accuracy: 54.64%\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.1607  Validation Accuracy: 54.88%\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss: 1.1738  Validation Accuracy: 55.02%\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss: 1.1631  Validation Accuracy: 55.26%\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss: 1.1329  Validation Accuracy: 55.18%\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss: 1.1710  Validation Accuracy: 55.16%\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.1541  Validation Accuracy: 55.68%\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss: 1.1502  Validation Accuracy: 55.82%\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss: 1.1752  Validation Accuracy: 53.72%\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss: 1.1320  Validation Accuracy: 54.92%\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss: 1.1550  Validation Accuracy: 55.28%\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.1429  Validation Accuracy: 55.98%\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss: 1.1573  Validation Accuracy: 55.18%\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss: 1.1413  Validation Accuracy: 54.94%\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss: 1.0952  Validation Accuracy: 55.82%\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss: 1.1314  Validation Accuracy: 56.02%\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.1270  Validation Accuracy: 56.30%\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss: 1.1475  Validation Accuracy: 55.72%\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss: 1.1385  Validation Accuracy: 55.30%\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss: 1.0986  Validation Accuracy: 55.60%\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss: 1.1311  Validation Accuracy: 55.70%\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.1232  Validation Accuracy: 55.86%\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss: 1.1249  Validation Accuracy: 55.98%\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss: 1.1164  Validation Accuracy: 55.56%\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss: 1.1232  Validation Accuracy: 55.02%\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss: 1.1115  Validation Accuracy: 55.86%\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.1028  Validation Accuracy: 56.52%\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss: 1.1086  Validation Accuracy: 56.62%\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss: 1.1110  Validation Accuracy: 55.96%\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss: 1.0893  Validation Accuracy: 55.86%\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss: 1.1007  Validation Accuracy: 56.58%\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.0958  Validation Accuracy: 56.80%\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss: 1.0831  Validation Accuracy: 56.98%\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss: 1.0958  Validation Accuracy: 55.54%\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss: 1.0864  Validation Accuracy: 56.14%\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss: 1.1077  Validation Accuracy: 55.46%\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.0855  Validation Accuracy: 56.42%\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss: 1.0764  Validation Accuracy: 57.38%\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss: 1.0811  Validation Accuracy: 55.86%\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss: 1.0691  Validation Accuracy: 55.90%\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss: 1.0747  Validation Accuracy: 56.74%\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 1.0783  Validation Accuracy: 57.40%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, CIFAR-10 Batch 2:  Loss: 1.0577  Validation Accuracy: 57.00%\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss: 1.0642  Validation Accuracy: 56.32%\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss: 1.0361  Validation Accuracy: 56.86%\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss: 1.0619  Validation Accuracy: 56.50%\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 1.0538  Validation Accuracy: 57.32%\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss: 1.0624  Validation Accuracy: 57.48%\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss: 1.0787  Validation Accuracy: 56.14%\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss: 1.0515  Validation Accuracy: 56.20%\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss: 1.0672  Validation Accuracy: 56.90%\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 1.0469  Validation Accuracy: 57.12%\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss: 1.0460  Validation Accuracy: 57.62%\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss: 1.0332  Validation Accuracy: 56.70%\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss: 1.0180  Validation Accuracy: 57.22%\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss: 1.0307  Validation Accuracy: 57.36%\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 1.0446  Validation Accuracy: 57.14%\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss: 1.0472  Validation Accuracy: 57.02%\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss: 1.0106  Validation Accuracy: 57.00%\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss: 1.0139  Validation Accuracy: 57.36%\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss: 1.0252  Validation Accuracy: 57.38%\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 1.0348  Validation Accuracy: 57.40%\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss: 1.0168  Validation Accuracy: 58.26%\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss: 1.0028  Validation Accuracy: 57.28%\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss: 1.0016  Validation Accuracy: 57.26%\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss: 1.0284  Validation Accuracy: 57.74%\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 1.0121  Validation Accuracy: 57.08%\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss: 1.0489  Validation Accuracy: 57.02%\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss: 1.0211  Validation Accuracy: 56.34%\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss: 0.9958  Validation Accuracy: 56.98%\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss: 0.9915  Validation Accuracy: 57.82%\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 1.0042  Validation Accuracy: 58.02%\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss: 1.0087  Validation Accuracy: 57.84%\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss: 1.0066  Validation Accuracy: 56.76%\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss: 0.9916  Validation Accuracy: 57.34%\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss: 0.9958  Validation Accuracy: 57.02%\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 1.0054  Validation Accuracy: 58.10%\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss: 1.0013  Validation Accuracy: 57.68%\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss: 1.0200  Validation Accuracy: 56.92%\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss: 0.9734  Validation Accuracy: 57.62%\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss: 0.9809  Validation Accuracy: 58.38%\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.9933  Validation Accuracy: 57.82%\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss: 0.9926  Validation Accuracy: 57.74%\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss: 0.9686  Validation Accuracy: 58.24%\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss: 0.9455  Validation Accuracy: 58.30%\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss: 0.9667  Validation Accuracy: 58.36%\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.9800  Validation Accuracy: 58.08%\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss: 0.9942  Validation Accuracy: 57.82%\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss: 1.0196  Validation Accuracy: 56.90%\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss: 0.9513  Validation Accuracy: 58.00%\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss: 0.9965  Validation Accuracy: 57.16%\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.9734  Validation Accuracy: 58.12%\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss: 0.9663  Validation Accuracy: 58.38%\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss: 0.9727  Validation Accuracy: 58.32%\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss: 0.9417  Validation Accuracy: 58.00%\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss: 0.9489  Validation Accuracy: 58.22%\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.9692  Validation Accuracy: 58.90%\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss: 0.9496  Validation Accuracy: 58.44%\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss: 0.9536  Validation Accuracy: 58.38%\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss: 0.9208  Validation Accuracy: 58.74%\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss: 0.9358  Validation Accuracy: 58.88%\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 0.9529  Validation Accuracy: 59.10%\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss: 0.9296  Validation Accuracy: 59.16%\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss: 0.9678  Validation Accuracy: 58.36%\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss: 0.9216  Validation Accuracy: 58.36%\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss: 0.9398  Validation Accuracy: 58.50%\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.9587  Validation Accuracy: 58.90%\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss: 0.9291  Validation Accuracy: 58.56%\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss: 0.9665  Validation Accuracy: 57.62%\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss: 0.9154  Validation Accuracy: 58.08%\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss: 0.9217  Validation Accuracy: 59.20%\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.9387  Validation Accuracy: 58.52%\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss: 0.9229  Validation Accuracy: 58.80%\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss: 0.9331  Validation Accuracy: 58.72%\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss: 0.9003  Validation Accuracy: 58.54%\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss: 0.9084  Validation Accuracy: 59.08%\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.9322  Validation Accuracy: 58.36%\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss: 0.9155  Validation Accuracy: 58.70%\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss: 0.9101  Validation Accuracy: 58.60%\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss: 0.9076  Validation Accuracy: 58.54%\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss: 0.9189  Validation Accuracy: 58.66%\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.9212  Validation Accuracy: 58.54%\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss: 0.9195  Validation Accuracy: 58.42%\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss: 0.9084  Validation Accuracy: 58.34%\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss: 0.8842  Validation Accuracy: 59.20%\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss: 0.8929  Validation Accuracy: 59.34%\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.9250  Validation Accuracy: 58.62%\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss: 0.9082  Validation Accuracy: 59.08%\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss: 0.8919  Validation Accuracy: 58.70%\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss: 0.8881  Validation Accuracy: 58.38%\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss: 0.8934  Validation Accuracy: 59.26%\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.9038  Validation Accuracy: 58.60%\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss: 0.8898  Validation Accuracy: 58.88%\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss: 0.8830  Validation Accuracy: 58.72%\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss: 0.8732  Validation Accuracy: 58.90%\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss: 0.8949  Validation Accuracy: 59.16%\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.9020  Validation Accuracy: 59.00%\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss: 0.9088  Validation Accuracy: 58.42%\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss: 0.8796  Validation Accuracy: 58.24%\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss: 0.8689  Validation Accuracy: 58.84%\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss: 0.8873  Validation Accuracy: 58.78%\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.9145  Validation Accuracy: 58.66%\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss: 0.8711  Validation Accuracy: 59.28%\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss: 0.8616  Validation Accuracy: 58.60%\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss: 0.8574  Validation Accuracy: 58.66%\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss: 0.8709  Validation Accuracy: 59.30%\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.9014  Validation Accuracy: 58.44%\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss: 0.8797  Validation Accuracy: 59.30%\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss: 0.8920  Validation Accuracy: 58.92%\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss: 0.8526  Validation Accuracy: 59.12%\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss: 0.8616  Validation Accuracy: 59.10%\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.9033  Validation Accuracy: 58.42%\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss: 0.8911  Validation Accuracy: 59.06%\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss: 0.8506  Validation Accuracy: 59.04%\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss: 0.8473  Validation Accuracy: 58.70%\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss: 0.8658  Validation Accuracy: 58.56%\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.8892  Validation Accuracy: 59.18%\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss: 0.8799  Validation Accuracy: 58.94%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, CIFAR-10 Batch 3:  Loss: 0.8702  Validation Accuracy: 58.78%\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss: 0.8498  Validation Accuracy: 58.60%\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss: 0.8523  Validation Accuracy: 58.76%\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.8825  Validation Accuracy: 58.62%\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss: 0.8654  Validation Accuracy: 59.38%\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss: 0.8546  Validation Accuracy: 58.80%\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss: 0.8676  Validation Accuracy: 58.28%\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss: 0.8666  Validation Accuracy: 58.86%\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.8723  Validation Accuracy: 58.98%\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss: 0.8533  Validation Accuracy: 59.26%\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss: 0.8394  Validation Accuracy: 59.24%\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss: 0.8639  Validation Accuracy: 57.86%\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss: 0.8474  Validation Accuracy: 59.60%\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.8765  Validation Accuracy: 59.04%\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss: 0.8500  Validation Accuracy: 59.10%\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss: 0.8396  Validation Accuracy: 59.06%\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss: 0.8357  Validation Accuracy: 58.96%\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss: 0.8527  Validation Accuracy: 59.46%\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.8585  Validation Accuracy: 59.22%\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss: 0.8689  Validation Accuracy: 59.36%\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss: 0.8374  Validation Accuracy: 59.62%\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss: 0.8289  Validation Accuracy: 58.74%\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss: 0.8426  Validation Accuracy: 59.32%\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.8511  Validation Accuracy: 59.30%\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss: 0.8444  Validation Accuracy: 59.86%\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss: 0.8518  Validation Accuracy: 58.92%\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss: 0.8404  Validation Accuracy: 58.78%\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss: 0.8382  Validation Accuracy: 59.12%\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.8505  Validation Accuracy: 59.74%\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss: 0.8280  Validation Accuracy: 59.94%\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss: 0.8518  Validation Accuracy: 59.28%\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss: 0.8186  Validation Accuracy: 58.78%\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss: 0.8541  Validation Accuracy: 59.00%\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.8463  Validation Accuracy: 60.00%\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss: 0.8124  Validation Accuracy: 59.78%\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss: 0.8473  Validation Accuracy: 59.02%\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss: 0.8136  Validation Accuracy: 59.14%\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss: 0.8558  Validation Accuracy: 58.74%\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.8528  Validation Accuracy: 59.56%\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss: 0.8109  Validation Accuracy: 59.98%\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss: 0.8334  Validation Accuracy: 59.18%\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss: 0.8262  Validation Accuracy: 58.54%\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss: 0.8723  Validation Accuracy: 58.60%\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.8505  Validation Accuracy: 59.16%\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss: 0.8289  Validation Accuracy: 60.14%\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss: 0.8160  Validation Accuracy: 59.26%\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss: 0.7992  Validation Accuracy: 59.00%\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss: 0.8338  Validation Accuracy: 59.06%\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.8552  Validation Accuracy: 59.00%\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss: 0.8315  Validation Accuracy: 59.78%\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss: 0.8844  Validation Accuracy: 58.82%\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss: 0.8303  Validation Accuracy: 58.52%\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss: 0.8367  Validation Accuracy: 59.00%\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.8328  Validation Accuracy: 60.20%\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss: 0.8189  Validation Accuracy: 59.80%\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss: 0.8363  Validation Accuracy: 59.50%\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss: 0.8187  Validation Accuracy: 58.68%\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss: 0.8311  Validation Accuracy: 58.74%\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.8428  Validation Accuracy: 59.74%\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss: 0.8372  Validation Accuracy: 58.42%\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss: 0.8181  Validation Accuracy: 59.56%\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss: 0.8193  Validation Accuracy: 59.00%\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss: 0.8094  Validation Accuracy: 59.34%\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.8347  Validation Accuracy: 60.18%\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss: 0.8381  Validation Accuracy: 58.26%\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss: 0.8250  Validation Accuracy: 59.80%\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss: 0.7933  Validation Accuracy: 59.30%\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss: 0.8129  Validation Accuracy: 59.36%\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.8226  Validation Accuracy: 60.26%\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss: 0.8399  Validation Accuracy: 58.36%\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss: 0.8159  Validation Accuracy: 58.86%\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss: 0.7858  Validation Accuracy: 59.50%\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss: 0.8087  Validation Accuracy: 59.60%\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.8402  Validation Accuracy: 59.80%\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss: 0.8118  Validation Accuracy: 59.06%\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss: 0.8046  Validation Accuracy: 59.40%\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss: 0.7896  Validation Accuracy: 59.76%\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss: 0.8317  Validation Accuracy: 58.46%\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.8268  Validation Accuracy: 60.12%\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss: 0.7962  Validation Accuracy: 59.34%\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss: 0.7867  Validation Accuracy: 60.06%\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss: 0.7895  Validation Accuracy: 59.86%\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss: 0.8176  Validation Accuracy: 59.20%\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.8120  Validation Accuracy: 60.06%\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss: 0.7878  Validation Accuracy: 59.64%\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss: 0.7604  Validation Accuracy: 60.06%\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss: 0.8080  Validation Accuracy: 58.74%\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss: 0.7933  Validation Accuracy: 59.48%\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.8272  Validation Accuracy: 59.12%\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss: 0.7965  Validation Accuracy: 59.66%\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss: 0.7691  Validation Accuracy: 59.90%\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss: 0.8048  Validation Accuracy: 58.94%\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss: 0.7786  Validation Accuracy: 59.34%\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.8141  Validation Accuracy: 59.64%\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss: 0.7800  Validation Accuracy: 59.86%\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss: 0.7540  Validation Accuracy: 60.12%\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss: 0.7983  Validation Accuracy: 58.94%\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss: 0.7651  Validation Accuracy: 59.96%\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.8017  Validation Accuracy: 59.62%\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss: 0.7740  Validation Accuracy: 59.32%\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss: 0.7399  Validation Accuracy: 60.68%\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss: 0.7836  Validation Accuracy: 59.18%\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss: 0.7718  Validation Accuracy: 60.12%\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.7921  Validation Accuracy: 59.60%\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss: 0.7806  Validation Accuracy: 59.64%\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss: 0.7469  Validation Accuracy: 60.00%\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss: 0.8093  Validation Accuracy: 58.50%\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss: 0.8033  Validation Accuracy: 59.46%\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.8143  Validation Accuracy: 58.98%\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss: 0.8015  Validation Accuracy: 58.92%\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss: 0.7432  Validation Accuracy: 59.90%\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss: 0.7841  Validation Accuracy: 59.22%\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss: 0.7765  Validation Accuracy: 59.76%\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.8021  Validation Accuracy: 59.48%\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss: 0.7660  Validation Accuracy: 59.30%\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss: 0.7470  Validation Accuracy: 60.40%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70, CIFAR-10 Batch 4:  Loss: 0.7731  Validation Accuracy: 59.52%\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss: 0.7666  Validation Accuracy: 60.18%\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.7772  Validation Accuracy: 60.38%\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss: 0.7436  Validation Accuracy: 59.82%\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss: 0.7255  Validation Accuracy: 60.64%\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss: 0.7724  Validation Accuracy: 59.00%\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss: 0.7671  Validation Accuracy: 59.84%\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.7963  Validation Accuracy: 59.70%\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss: 0.7463  Validation Accuracy: 59.86%\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss: 0.7255  Validation Accuracy: 60.38%\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss: 0.7711  Validation Accuracy: 58.96%\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss: 0.7523  Validation Accuracy: 60.02%\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.7753  Validation Accuracy: 59.48%\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss: 0.7549  Validation Accuracy: 60.00%\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss: 0.7149  Validation Accuracy: 60.66%\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss: 0.7434  Validation Accuracy: 59.96%\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss: 0.7414  Validation Accuracy: 60.54%\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.7631  Validation Accuracy: 59.98%\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss: 0.7442  Validation Accuracy: 59.68%\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss: 0.7109  Validation Accuracy: 60.60%\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss: 0.7397  Validation Accuracy: 59.72%\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss: 0.7329  Validation Accuracy: 60.26%\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.7440  Validation Accuracy: 60.38%\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss: 0.7338  Validation Accuracy: 60.10%\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss: 0.7088  Validation Accuracy: 60.44%\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss: 0.7278  Validation Accuracy: 60.04%\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss: 0.7352  Validation Accuracy: 60.46%\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.7531  Validation Accuracy: 59.72%\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss: 0.7366  Validation Accuracy: 59.92%\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss: 0.7168  Validation Accuracy: 60.56%\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss: 0.7181  Validation Accuracy: 60.04%\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss: 0.7254  Validation Accuracy: 60.40%\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.7385  Validation Accuracy: 60.30%\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss: 0.7191  Validation Accuracy: 60.22%\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss: 0.7132  Validation Accuracy: 60.48%\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss: 0.7176  Validation Accuracy: 59.52%\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss: 0.7318  Validation Accuracy: 60.02%\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.7397  Validation Accuracy: 60.00%\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss: 0.7113  Validation Accuracy: 60.20%\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss: 0.6987  Validation Accuracy: 60.68%\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss: 0.7172  Validation Accuracy: 60.12%\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss: 0.7170  Validation Accuracy: 60.20%\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.7354  Validation Accuracy: 60.12%\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss: 0.7214  Validation Accuracy: 59.84%\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss: 0.7196  Validation Accuracy: 60.38%\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss: 0.7077  Validation Accuracy: 59.32%\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss: 0.7213  Validation Accuracy: 60.12%\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.7338  Validation Accuracy: 60.10%\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss: 0.7015  Validation Accuracy: 60.46%\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss: 0.6947  Validation Accuracy: 60.76%\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss: 0.7027  Validation Accuracy: 60.38%\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss: 0.7230  Validation Accuracy: 60.04%\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.7297  Validation Accuracy: 60.34%\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss: 0.6924  Validation Accuracy: 60.44%\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss: 0.6966  Validation Accuracy: 60.90%\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss: 0.7005  Validation Accuracy: 60.28%\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss: 0.7191  Validation Accuracy: 60.28%\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.7172  Validation Accuracy: 60.48%\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss: 0.6976  Validation Accuracy: 60.18%\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss: 0.6995  Validation Accuracy: 60.34%\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss: 0.6925  Validation Accuracy: 60.20%\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss: 0.7142  Validation Accuracy: 60.22%\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.7186  Validation Accuracy: 60.40%\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss: 0.6847  Validation Accuracy: 60.96%\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss: 0.6914  Validation Accuracy: 60.64%\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss: 0.6948  Validation Accuracy: 60.58%\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss: 0.7125  Validation Accuracy: 60.16%\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.7142  Validation Accuracy: 60.74%\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss: 0.6788  Validation Accuracy: 60.36%\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss: 0.6793  Validation Accuracy: 60.94%\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss: 0.7076  Validation Accuracy: 59.74%\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss: 0.7161  Validation Accuracy: 60.16%\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.7329  Validation Accuracy: 59.80%\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss: 0.6875  Validation Accuracy: 60.06%\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss: 0.6852  Validation Accuracy: 61.08%\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss: 0.6976  Validation Accuracy: 60.30%\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss: 0.7052  Validation Accuracy: 60.00%\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.7165  Validation Accuracy: 60.14%\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss: 0.6701  Validation Accuracy: 60.66%\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss: 0.6783  Validation Accuracy: 60.84%\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss: 0.6748  Validation Accuracy: 60.18%\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss: 0.6975  Validation Accuracy: 60.16%\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.6966  Validation Accuracy: 60.44%\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss: 0.6794  Validation Accuracy: 60.58%\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss: 0.6735  Validation Accuracy: 60.72%\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss: 0.6831  Validation Accuracy: 59.82%\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss: 0.7025  Validation Accuracy: 60.12%\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.7141  Validation Accuracy: 59.86%\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss: 0.6741  Validation Accuracy: 60.68%\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss: 0.6732  Validation Accuracy: 60.64%\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss: 0.6678  Validation Accuracy: 59.72%\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss: 0.7075  Validation Accuracy: 59.90%\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.7099  Validation Accuracy: 60.00%\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss: 0.6691  Validation Accuracy: 60.52%\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss: 0.6770  Validation Accuracy: 60.88%\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss: 0.6657  Validation Accuracy: 60.56%\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss: 0.6913  Validation Accuracy: 60.08%\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.7082  Validation Accuracy: 59.86%\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss: 0.6738  Validation Accuracy: 60.64%\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss: 0.6815  Validation Accuracy: 60.34%\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss: 0.6864  Validation Accuracy: 59.40%\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss: 0.6941  Validation Accuracy: 59.94%\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.6992  Validation Accuracy: 60.12%\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss: 0.6785  Validation Accuracy: 59.82%\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss: 0.6691  Validation Accuracy: 60.44%\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss: 0.6608  Validation Accuracy: 59.42%\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss: 0.7070  Validation Accuracy: 60.54%\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.7050  Validation Accuracy: 59.18%\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss: 0.6808  Validation Accuracy: 60.26%\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss: 0.6605  Validation Accuracy: 60.56%\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss: 0.6546  Validation Accuracy: 59.86%\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss: 0.6968  Validation Accuracy: 60.76%\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.7079  Validation Accuracy: 59.18%\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss: 0.6992  Validation Accuracy: 60.02%\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss: 0.6428  Validation Accuracy: 60.80%\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss: 0.6664  Validation Accuracy: 59.72%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, CIFAR-10 Batch 5:  Loss: 0.6804  Validation Accuracy: 60.28%\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.7065  Validation Accuracy: 59.76%\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss: 0.6963  Validation Accuracy: 60.44%\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss: 0.6400  Validation Accuracy: 61.08%\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss: 0.6539  Validation Accuracy: 60.30%\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss: 0.6733  Validation Accuracy: 60.62%\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.7033  Validation Accuracy: 59.96%\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss: 0.6738  Validation Accuracy: 60.92%\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss: 0.6689  Validation Accuracy: 60.58%\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss: 0.6748  Validation Accuracy: 60.10%\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss: 0.6749  Validation Accuracy: 60.56%\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.6892  Validation Accuracy: 60.42%\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss: 0.6738  Validation Accuracy: 60.48%\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss: 0.6796  Validation Accuracy: 60.36%\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss: 0.6823  Validation Accuracy: 59.46%\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss: 0.6842  Validation Accuracy: 59.68%\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.7111  Validation Accuracy: 59.38%\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss: 0.6862  Validation Accuracy: 59.58%\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss: 0.6391  Validation Accuracy: 61.42%\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss: 0.6968  Validation Accuracy: 59.20%\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss: 0.6894  Validation Accuracy: 60.26%\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.7018  Validation Accuracy: 59.90%\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss: 0.6969  Validation Accuracy: 59.20%\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss: 0.6826  Validation Accuracy: 59.88%\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss: 0.6695  Validation Accuracy: 59.86%\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss: 0.6919  Validation Accuracy: 60.26%\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.7043  Validation Accuracy: 61.22%\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss: 0.6674  Validation Accuracy: 59.94%\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss: 0.6572  Validation Accuracy: 60.52%\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss: 0.6450  Validation Accuracy: 60.58%\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss: 0.6770  Validation Accuracy: 60.70%\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.6946  Validation Accuracy: 60.34%\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss: 0.6808  Validation Accuracy: 59.28%\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss: 0.6539  Validation Accuracy: 60.42%\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss: 0.6470  Validation Accuracy: 60.42%\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss: 0.6704  Validation Accuracy: 60.58%\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss: 0.6914  Validation Accuracy: 60.36%\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss: 0.6688  Validation Accuracy: 59.58%\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss: 0.6559  Validation Accuracy: 60.18%\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss: 0.6503  Validation Accuracy: 59.82%\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss: 0.6699  Validation Accuracy: 60.52%\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss: 0.6789  Validation Accuracy: 59.76%\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss: 0.6578  Validation Accuracy: 60.02%\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss: 0.6609  Validation Accuracy: 60.70%\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss: 0.6459  Validation Accuracy: 60.82%\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss: 0.6472  Validation Accuracy: 60.88%\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss: 0.6695  Validation Accuracy: 60.50%\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss: 0.6461  Validation Accuracy: 59.72%\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss: 0.6386  Validation Accuracy: 60.88%\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss: 0.6235  Validation Accuracy: 60.92%\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss: 0.6414  Validation Accuracy: 60.92%\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss: 0.6540  Validation Accuracy: 60.30%\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss: 0.6323  Validation Accuracy: 60.22%\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss: 0.6498  Validation Accuracy: 60.52%\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss: 0.6298  Validation Accuracy: 60.42%\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss: 0.6347  Validation Accuracy: 60.88%\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss: 0.6605  Validation Accuracy: 60.22%\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss: 0.6401  Validation Accuracy: 59.32%\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss: 0.6439  Validation Accuracy: 60.54%\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss: 0.6297  Validation Accuracy: 59.62%\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss: 0.6574  Validation Accuracy: 60.18%\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss: 0.6599  Validation Accuracy: 60.38%\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss: 0.6436  Validation Accuracy: 60.06%\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss: 0.6485  Validation Accuracy: 61.18%\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss: 0.6201  Validation Accuracy: 60.18%\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss: 0.6423  Validation Accuracy: 60.40%\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss: 0.6536  Validation Accuracy: 60.20%\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss: 0.6534  Validation Accuracy: 60.06%\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss: 0.6360  Validation Accuracy: 60.44%\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss: 0.6236  Validation Accuracy: 59.44%\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss: 0.6495  Validation Accuracy: 60.28%\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss: 0.6386  Validation Accuracy: 60.82%\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss: 0.6374  Validation Accuracy: 59.78%\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss: 0.6389  Validation Accuracy: 60.54%\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss: 0.5931  Validation Accuracy: 60.60%\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss: 0.6370  Validation Accuracy: 60.64%\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss: 0.6390  Validation Accuracy: 60.58%\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss: 0.6265  Validation Accuracy: 60.00%\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss: 0.6398  Validation Accuracy: 60.88%\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss: 0.6085  Validation Accuracy: 60.16%\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss: 0.6412  Validation Accuracy: 60.54%\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss: 0.6522  Validation Accuracy: 59.40%\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss: 0.6386  Validation Accuracy: 59.24%\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss: 0.6515  Validation Accuracy: 60.72%\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss: 0.6310  Validation Accuracy: 59.36%\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss: 0.6325  Validation Accuracy: 60.72%\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss: 0.6322  Validation Accuracy: 60.44%\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss: 0.6299  Validation Accuracy: 60.32%\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss: 0.6193  Validation Accuracy: 60.64%\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss: 0.6170  Validation Accuracy: 60.06%\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss: 0.6630  Validation Accuracy: 60.56%\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss: 0.6371  Validation Accuracy: 60.20%\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss: 0.6287  Validation Accuracy: 60.34%\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss: 0.6489  Validation Accuracy: 60.52%\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss: 0.5945  Validation Accuracy: 60.28%\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss: 0.6300  Validation Accuracy: 60.56%\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss: 0.6202  Validation Accuracy: 60.80%\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss: 0.6159  Validation Accuracy: 59.80%\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss: 0.6332  Validation Accuracy: 60.44%\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss: 0.6088  Validation Accuracy: 60.56%\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss: 0.6669  Validation Accuracy: 60.12%\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss: 0.6420  Validation Accuracy: 60.04%\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss: 0.6232  Validation Accuracy: 59.80%\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss: 0.6186  Validation Accuracy: 60.86%\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss: 0.6184  Validation Accuracy: 60.64%\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss: 0.6626  Validation Accuracy: 60.12%\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss: 0.6511  Validation Accuracy: 59.90%\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss: 0.6247  Validation Accuracy: 60.42%\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss: 0.6019  Validation Accuracy: 60.62%\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss: 0.6011  Validation Accuracy: 60.76%\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss: 0.6460  Validation Accuracy: 60.32%\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss: 0.6494  Validation Accuracy: 60.52%\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss: 0.6357  Validation Accuracy: 59.78%\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss: 0.6093  Validation Accuracy: 60.66%\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss: 0.6263  Validation Accuracy: 60.02%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116, CIFAR-10 Batch 5:  Loss: 0.6344  Validation Accuracy: 60.10%\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss: 0.6278  Validation Accuracy: 60.80%\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss: 0.6087  Validation Accuracy: 60.44%\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss: 0.5930  Validation Accuracy: 60.86%\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss: 0.6217  Validation Accuracy: 60.14%\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss: 0.6321  Validation Accuracy: 60.96%\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss: 0.6259  Validation Accuracy: 60.96%\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss: 0.6291  Validation Accuracy: 59.72%\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss: 0.5923  Validation Accuracy: 61.22%\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss: 0.5971  Validation Accuracy: 60.76%\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss: 0.6107  Validation Accuracy: 60.76%\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss: 0.6446  Validation Accuracy: 60.60%\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss: 0.6349  Validation Accuracy: 59.64%\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss: 0.6108  Validation Accuracy: 60.44%\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss: 0.5893  Validation Accuracy: 60.20%\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss: 0.6104  Validation Accuracy: 60.74%\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss: 0.6142  Validation Accuracy: 60.64%\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss: 0.6135  Validation Accuracy: 59.94%\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss: 0.6228  Validation Accuracy: 60.22%\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss: 0.6016  Validation Accuracy: 60.22%\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss: 0.6088  Validation Accuracy: 60.62%\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss: 0.6107  Validation Accuracy: 60.68%\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss: 0.6000  Validation Accuracy: 60.38%\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss: 0.6173  Validation Accuracy: 60.56%\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss: 0.6130  Validation Accuracy: 59.52%\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss: 0.6167  Validation Accuracy: 60.86%\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss: 0.6139  Validation Accuracy: 60.76%\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss: 0.6006  Validation Accuracy: 59.88%\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss: 0.6295  Validation Accuracy: 59.72%\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss: 0.6095  Validation Accuracy: 60.46%\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss: 0.6137  Validation Accuracy: 61.22%\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss: 0.6196  Validation Accuracy: 60.50%\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss: 0.5870  Validation Accuracy: 60.86%\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss: 0.6162  Validation Accuracy: 60.28%\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss: 0.6009  Validation Accuracy: 59.58%\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss: 0.6309  Validation Accuracy: 60.38%\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss: 0.6052  Validation Accuracy: 60.80%\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss: 0.5891  Validation Accuracy: 60.16%\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss: 0.5991  Validation Accuracy: 60.78%\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss: 0.6032  Validation Accuracy: 59.66%\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss: 0.6431  Validation Accuracy: 59.88%\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss: 0.6079  Validation Accuracy: 60.26%\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss: 0.5888  Validation Accuracy: 60.62%\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss: 0.6187  Validation Accuracy: 60.86%\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss: 0.5839  Validation Accuracy: 59.78%\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss: 0.6233  Validation Accuracy: 60.10%\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss: 0.6269  Validation Accuracy: 59.34%\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss: 0.6047  Validation Accuracy: 60.10%\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss: 0.5958  Validation Accuracy: 60.60%\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss: 0.6033  Validation Accuracy: 60.24%\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss: 0.6356  Validation Accuracy: 59.50%\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss: 0.6232  Validation Accuracy: 59.22%\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss: 0.5855  Validation Accuracy: 60.38%\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss: 0.6005  Validation Accuracy: 60.68%\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss: 0.6289  Validation Accuracy: 59.46%\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss: 0.6375  Validation Accuracy: 58.70%\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss: 0.6427  Validation Accuracy: 58.94%\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss: 0.6569  Validation Accuracy: 59.30%\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss: 0.6053  Validation Accuracy: 59.70%\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss: 0.6386  Validation Accuracy: 59.40%\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss: 0.6402  Validation Accuracy: 59.60%\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss: 0.6810  Validation Accuracy: 58.82%\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss: 0.6435  Validation Accuracy: 59.46%\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss: 0.6024  Validation Accuracy: 60.02%\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss: 0.6555  Validation Accuracy: 59.84%\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss: 0.6147  Validation Accuracy: 60.28%\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss: 0.5946  Validation Accuracy: 60.68%\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss: 0.5836  Validation Accuracy: 60.70%\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss: 0.5809  Validation Accuracy: 60.68%\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss: 0.6471  Validation Accuracy: 59.68%\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss: 0.6074  Validation Accuracy: 60.52%\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss: 0.6063  Validation Accuracy: 60.30%\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss: 0.5976  Validation Accuracy: 61.04%\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss: 0.5881  Validation Accuracy: 60.40%\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss: 0.6149  Validation Accuracy: 59.86%\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss: 0.6027  Validation Accuracy: 60.04%\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss: 0.6280  Validation Accuracy: 59.20%\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss: 0.5994  Validation Accuracy: 60.68%\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss: 0.5946  Validation Accuracy: 59.82%\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss: 0.5843  Validation Accuracy: 60.74%\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss: 0.6698  Validation Accuracy: 59.60%\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss: 0.6358  Validation Accuracy: 59.10%\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss: 0.6096  Validation Accuracy: 60.46%\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss: 0.5933  Validation Accuracy: 60.30%\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss: 0.5910  Validation Accuracy: 59.96%\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss: 0.6474  Validation Accuracy: 59.50%\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss: 0.6143  Validation Accuracy: 60.28%\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss: 0.6069  Validation Accuracy: 59.38%\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss: 0.5979  Validation Accuracy: 59.82%\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss: 0.6083  Validation Accuracy: 59.70%\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss: 0.6448  Validation Accuracy: 59.00%\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss: 0.6120  Validation Accuracy: 60.00%\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss: 0.6089  Validation Accuracy: 59.24%\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss: 0.6159  Validation Accuracy: 59.68%\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss: 0.5880  Validation Accuracy: 59.52%\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss: 0.6932  Validation Accuracy: 58.30%\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss: 0.5978  Validation Accuracy: 60.24%\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss: 0.6447  Validation Accuracy: 58.68%\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss: 0.6030  Validation Accuracy: 60.12%\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss: 0.6375  Validation Accuracy: 58.70%\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss: 0.6415  Validation Accuracy: 59.64%\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss: 0.5909  Validation Accuracy: 60.22%\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss: 0.6066  Validation Accuracy: 58.94%\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss: 0.5991  Validation Accuracy: 60.00%\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss: 0.6074  Validation Accuracy: 59.86%\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss: 0.6452  Validation Accuracy: 59.36%\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss: 0.6113  Validation Accuracy: 59.86%\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss: 0.6329  Validation Accuracy: 57.76%\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss: 0.5900  Validation Accuracy: 60.28%\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss: 0.6270  Validation Accuracy: 59.48%\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss: 0.6221  Validation Accuracy: 59.66%\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss: 0.6172  Validation Accuracy: 59.18%\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss: 0.6309  Validation Accuracy: 59.34%\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss: 0.6055  Validation Accuracy: 60.28%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139, CIFAR-10 Batch 4:  Loss: 0.6172  Validation Accuracy: 59.76%\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss: 0.6229  Validation Accuracy: 59.88%\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss: 0.6097  Validation Accuracy: 59.90%\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss: 0.5933  Validation Accuracy: 59.60%\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss: 0.5960  Validation Accuracy: 59.14%\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss: 0.5854  Validation Accuracy: 60.18%\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss: 0.6360  Validation Accuracy: 59.26%\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss: 0.5972  Validation Accuracy: 60.56%\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss: 0.5749  Validation Accuracy: 60.44%\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss: 0.6066  Validation Accuracy: 59.32%\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss: 0.6107  Validation Accuracy: 60.38%\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss: 0.6274  Validation Accuracy: 60.04%\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss: 0.6102  Validation Accuracy: 60.08%\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss: 0.5942  Validation Accuracy: 59.30%\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss: 0.6307  Validation Accuracy: 58.58%\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss: 0.6208  Validation Accuracy: 59.04%\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss: 0.6315  Validation Accuracy: 59.10%\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss: 0.6012  Validation Accuracy: 59.72%\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss: 0.5898  Validation Accuracy: 60.10%\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss: 0.6317  Validation Accuracy: 58.78%\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss: 0.6369  Validation Accuracy: 58.94%\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss: 0.6416  Validation Accuracy: 59.48%\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss: 0.6335  Validation Accuracy: 59.90%\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss: 0.6319  Validation Accuracy: 59.56%\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss: 0.6205  Validation Accuracy: 59.82%\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss: 0.6414  Validation Accuracy: 59.16%\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss: 0.6342  Validation Accuracy: 60.12%\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss: 0.6173  Validation Accuracy: 59.24%\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss: 0.6243  Validation Accuracy: 59.02%\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss: 0.6135  Validation Accuracy: 59.90%\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss: 0.6248  Validation Accuracy: 58.54%\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss: 0.6052  Validation Accuracy: 60.12%\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss: 0.6136  Validation Accuracy: 58.90%\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss: 0.6049  Validation Accuracy: 59.32%\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss: 0.6166  Validation Accuracy: 60.06%\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss: 0.6379  Validation Accuracy: 59.32%\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss: 0.6137  Validation Accuracy: 60.58%\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss: 0.6220  Validation Accuracy: 58.92%\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss: 0.6087  Validation Accuracy: 59.74%\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss: 0.6359  Validation Accuracy: 59.74%\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss: 0.6429  Validation Accuracy: 59.46%\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss: 0.6159  Validation Accuracy: 60.18%\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss: 0.5896  Validation Accuracy: 59.60%\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss: 0.6062  Validation Accuracy: 59.36%\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss: 0.6116  Validation Accuracy: 59.88%\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss: 0.6016  Validation Accuracy: 59.92%\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss: 0.6187  Validation Accuracy: 60.02%\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss: 0.5998  Validation Accuracy: 59.72%\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss: 0.5669  Validation Accuracy: 60.20%\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss: 0.6099  Validation Accuracy: 59.78%\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss: 0.6018  Validation Accuracy: 59.88%\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss: 0.6047  Validation Accuracy: 59.82%\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss: 0.6055  Validation Accuracy: 59.04%\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss: 0.5825  Validation Accuracy: 59.56%\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss: 0.6079  Validation Accuracy: 59.96%\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss: 0.5791  Validation Accuracy: 60.02%\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss: 0.6282  Validation Accuracy: 59.44%\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss: 0.5893  Validation Accuracy: 59.22%\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss: 0.5871  Validation Accuracy: 59.36%\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss: 0.5743  Validation Accuracy: 60.18%\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss: 0.5655  Validation Accuracy: 60.70%\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss: 0.5987  Validation Accuracy: 59.74%\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss: 0.5939  Validation Accuracy: 59.76%\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss: 0.5767  Validation Accuracy: 59.68%\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss: 0.5937  Validation Accuracy: 60.26%\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss: 0.5540  Validation Accuracy: 61.26%\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss: 0.5881  Validation Accuracy: 60.20%\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss: 0.6235  Validation Accuracy: 59.24%\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss: 0.5886  Validation Accuracy: 58.88%\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss: 0.6154  Validation Accuracy: 59.62%\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss: 0.5622  Validation Accuracy: 60.36%\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss: 0.5980  Validation Accuracy: 60.50%\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss: 0.6144  Validation Accuracy: 59.78%\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss: 0.6249  Validation Accuracy: 58.84%\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss: 0.6233  Validation Accuracy: 59.34%\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss: 0.5917  Validation Accuracy: 59.82%\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss: 0.6188  Validation Accuracy: 60.20%\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss: 0.6343  Validation Accuracy: 59.44%\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss: 0.6311  Validation Accuracy: 58.38%\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss: 0.6421  Validation Accuracy: 59.16%\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss: 0.6130  Validation Accuracy: 59.26%\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss: 0.6604  Validation Accuracy: 59.20%\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss: 0.6700  Validation Accuracy: 59.16%\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss: 0.6720  Validation Accuracy: 57.44%\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss: 0.6607  Validation Accuracy: 59.38%\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss: 0.6245  Validation Accuracy: 59.76%\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss: 0.6419  Validation Accuracy: 58.82%\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss: 0.6332  Validation Accuracy: 58.96%\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss: 0.5934  Validation Accuracy: 59.88%\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss: 0.5992  Validation Accuracy: 60.12%\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss: 0.6066  Validation Accuracy: 60.16%\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss: 0.6376  Validation Accuracy: 59.04%\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss: 0.6172  Validation Accuracy: 59.62%\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss: 0.6256  Validation Accuracy: 58.68%\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss: 0.6052  Validation Accuracy: 59.94%\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss: 0.5727  Validation Accuracy: 60.96%\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss: 0.5998  Validation Accuracy: 59.50%\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss: 0.5943  Validation Accuracy: 60.22%\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss: 0.5849  Validation Accuracy: 59.90%\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss: 0.5985  Validation Accuracy: 60.24%\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss: 0.6037  Validation Accuracy: 60.50%\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss: 0.5959  Validation Accuracy: 60.92%\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss: 0.5760  Validation Accuracy: 60.98%\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss: 0.5887  Validation Accuracy: 59.62%\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss: 0.6109  Validation Accuracy: 60.16%\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss: 0.6214  Validation Accuracy: 60.34%\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss: 0.6281  Validation Accuracy: 60.44%\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss: 0.5947  Validation Accuracy: 60.58%\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss: 0.5952  Validation Accuracy: 60.60%\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss: 0.6062  Validation Accuracy: 59.84%\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss: 0.5914  Validation Accuracy: 59.92%\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss: 0.5975  Validation Accuracy: 60.42%\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss: 0.5721  Validation Accuracy: 60.50%\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss: 0.5567  Validation Accuracy: 61.04%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162, CIFAR-10 Batch 3:  Loss: 0.5927  Validation Accuracy: 60.94%\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss: 0.5820  Validation Accuracy: 60.34%\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss: 0.5865  Validation Accuracy: 60.36%\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss: 0.5673  Validation Accuracy: 60.70%\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss: 0.5765  Validation Accuracy: 60.64%\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss: 0.5825  Validation Accuracy: 60.86%\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss: 0.5636  Validation Accuracy: 60.82%\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss: 0.5845  Validation Accuracy: 60.60%\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss: 0.5574  Validation Accuracy: 60.56%\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss: 0.5484  Validation Accuracy: 60.62%\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss: 0.5705  Validation Accuracy: 60.80%\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss: 0.5531  Validation Accuracy: 60.64%\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss: 0.5630  Validation Accuracy: 61.02%\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss: 0.5710  Validation Accuracy: 60.32%\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss: 0.5541  Validation Accuracy: 60.14%\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss: 0.5613  Validation Accuracy: 61.00%\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss: 0.5396  Validation Accuracy: 60.72%\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss: 0.5614  Validation Accuracy: 60.58%\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss: 0.5497  Validation Accuracy: 60.84%\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss: 0.5316  Validation Accuracy: 61.02%\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss: 0.5494  Validation Accuracy: 60.56%\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss: 0.5418  Validation Accuracy: 60.86%\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss: 0.5627  Validation Accuracy: 60.78%\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss: 0.5744  Validation Accuracy: 60.04%\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss: 0.5415  Validation Accuracy: 60.34%\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss: 0.5418  Validation Accuracy: 61.16%\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss: 0.5368  Validation Accuracy: 60.88%\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss: 0.5469  Validation Accuracy: 60.70%\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss: 0.5391  Validation Accuracy: 60.06%\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss: 0.5411  Validation Accuracy: 60.38%\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss: 0.5466  Validation Accuracy: 60.86%\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss: 0.5454  Validation Accuracy: 60.68%\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss: 0.5521  Validation Accuracy: 60.50%\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss: 0.5362  Validation Accuracy: 60.82%\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss: 0.5239  Validation Accuracy: 61.44%\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss: 0.5468  Validation Accuracy: 60.84%\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss: 0.5485  Validation Accuracy: 60.42%\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss: 0.5499  Validation Accuracy: 60.48%\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss: 0.5583  Validation Accuracy: 60.08%\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss: 0.5321  Validation Accuracy: 60.34%\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss: 0.5474  Validation Accuracy: 60.42%\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss: 0.5271  Validation Accuracy: 60.62%\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss: 0.5510  Validation Accuracy: 60.64%\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss: 0.5594  Validation Accuracy: 60.00%\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss: 0.5392  Validation Accuracy: 60.30%\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss: 0.5476  Validation Accuracy: 60.80%\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss: 0.5569  Validation Accuracy: 60.10%\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss: 0.5554  Validation Accuracy: 60.50%\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss: 0.5405  Validation Accuracy: 60.46%\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss: 0.5154  Validation Accuracy: 60.66%\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss: 0.5322  Validation Accuracy: 60.94%\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss: 0.5261  Validation Accuracy: 60.38%\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss: 0.5461  Validation Accuracy: 60.24%\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss: 0.5618  Validation Accuracy: 60.02%\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss: 0.5398  Validation Accuracy: 60.46%\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss: 0.5340  Validation Accuracy: 60.74%\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss: 0.5408  Validation Accuracy: 60.14%\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss: 0.5511  Validation Accuracy: 59.80%\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss: 0.5405  Validation Accuracy: 59.74%\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss: 0.5294  Validation Accuracy: 60.28%\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss: 0.5266  Validation Accuracy: 60.82%\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss: 0.5103  Validation Accuracy: 60.56%\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss: 0.5445  Validation Accuracy: 60.02%\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss: 0.5427  Validation Accuracy: 59.98%\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss: 0.5080  Validation Accuracy: 60.96%\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss: 0.5147  Validation Accuracy: 61.20%\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss: 0.5199  Validation Accuracy: 60.28%\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss: 0.5404  Validation Accuracy: 60.34%\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss: 0.5362  Validation Accuracy: 60.44%\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss: 0.5287  Validation Accuracy: 60.44%\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss: 0.5253  Validation Accuracy: 60.42%\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss: 0.5197  Validation Accuracy: 60.08%\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss: 0.5464  Validation Accuracy: 60.38%\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss: 0.5358  Validation Accuracy: 60.50%\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss: 0.5193  Validation Accuracy: 60.14%\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss: 0.5081  Validation Accuracy: 61.02%\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss: 0.5028  Validation Accuracy: 60.52%\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss: 0.5559  Validation Accuracy: 59.58%\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss: 0.5507  Validation Accuracy: 60.40%\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss: 0.5111  Validation Accuracy: 60.74%\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss: 0.5371  Validation Accuracy: 60.86%\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss: 0.5225  Validation Accuracy: 60.04%\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss: 0.5422  Validation Accuracy: 59.88%\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss: 0.5710  Validation Accuracy: 60.00%\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss: 0.5168  Validation Accuracy: 61.08%\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss: 0.5207  Validation Accuracy: 60.86%\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss: 0.4932  Validation Accuracy: 60.98%\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss: 0.5224  Validation Accuracy: 60.30%\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss: 0.5313  Validation Accuracy: 60.20%\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss: 0.5310  Validation Accuracy: 59.88%\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss: 0.5382  Validation Accuracy: 60.68%\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss: 0.5124  Validation Accuracy: 60.40%\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss: 0.5392  Validation Accuracy: 60.62%\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss: 0.5417  Validation Accuracy: 59.90%\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss: 0.5397  Validation Accuracy: 59.86%\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss: 0.5353  Validation Accuracy: 60.68%\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss: 0.5440  Validation Accuracy: 60.34%\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss: 0.5226  Validation Accuracy: 60.58%\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss: 0.5229  Validation Accuracy: 59.84%\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss: 0.5146  Validation Accuracy: 59.86%\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss: 0.5183  Validation Accuracy: 61.06%\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss: 0.5484  Validation Accuracy: 60.08%\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss: 0.5412  Validation Accuracy: 60.68%\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss: 0.5307  Validation Accuracy: 60.12%\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss: 0.5230  Validation Accuracy: 59.78%\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss: 0.5122  Validation Accuracy: 60.88%\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss: 0.5428  Validation Accuracy: 59.24%\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss: 0.5773  Validation Accuracy: 60.42%\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss: 0.5527  Validation Accuracy: 60.08%\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss: 0.5255  Validation Accuracy: 59.76%\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss: 0.5212  Validation Accuracy: 60.54%\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss: 0.5495  Validation Accuracy: 59.46%\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss: 0.5447  Validation Accuracy: 60.86%\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss: 0.5243  Validation Accuracy: 59.84%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185, CIFAR-10 Batch 2:  Loss: 0.5445  Validation Accuracy: 59.84%\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss: 0.5564  Validation Accuracy: 60.82%\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss: 0.5503  Validation Accuracy: 59.62%\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss: 0.5340  Validation Accuracy: 60.80%\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss: 0.5248  Validation Accuracy: 60.24%\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss: 0.5194  Validation Accuracy: 60.40%\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss: 0.5309  Validation Accuracy: 60.44%\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss: 0.5168  Validation Accuracy: 60.14%\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss: 0.5463  Validation Accuracy: 59.84%\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss: 0.5381  Validation Accuracy: 59.70%\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss: 0.5153  Validation Accuracy: 61.06%\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss: 0.5277  Validation Accuracy: 60.64%\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss: 0.5175  Validation Accuracy: 60.18%\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss: 0.5376  Validation Accuracy: 60.12%\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss: 0.5408  Validation Accuracy: 59.60%\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss: 0.5315  Validation Accuracy: 60.64%\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss: 0.5419  Validation Accuracy: 60.62%\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss: 0.5709  Validation Accuracy: 59.06%\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss: 0.6063  Validation Accuracy: 59.10%\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss: 0.5628  Validation Accuracy: 59.40%\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss: 0.5418  Validation Accuracy: 60.86%\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss: 0.5726  Validation Accuracy: 59.18%\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss: 0.5386  Validation Accuracy: 59.86%\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss: 0.5517  Validation Accuracy: 59.58%\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss: 0.5925  Validation Accuracy: 59.04%\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss: 0.5296  Validation Accuracy: 60.68%\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss: 0.5412  Validation Accuracy: 60.02%\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss: 0.5339  Validation Accuracy: 60.20%\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss: 0.5759  Validation Accuracy: 58.94%\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss: 0.5765  Validation Accuracy: 58.90%\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss: 0.5423  Validation Accuracy: 59.94%\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss: 0.5442  Validation Accuracy: 60.24%\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss: 0.5265  Validation Accuracy: 60.16%\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss: 0.6052  Validation Accuracy: 58.84%\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss: 0.5458  Validation Accuracy: 59.30%\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss: 0.5504  Validation Accuracy: 60.38%\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss: 0.5565  Validation Accuracy: 59.76%\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss: 0.5216  Validation Accuracy: 60.72%\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss: 0.5718  Validation Accuracy: 59.00%\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss: 0.5564  Validation Accuracy: 59.32%\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss: 0.5251  Validation Accuracy: 59.90%\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss: 0.5394  Validation Accuracy: 60.38%\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss: 0.5225  Validation Accuracy: 60.00%\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss: 0.5655  Validation Accuracy: 59.36%\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss: 0.5701  Validation Accuracy: 58.20%\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss: 0.5380  Validation Accuracy: 59.54%\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss: 0.5353  Validation Accuracy: 60.64%\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss: 0.4939  Validation Accuracy: 60.70%\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss: 0.5775  Validation Accuracy: 58.86%\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss: 0.5681  Validation Accuracy: 58.52%\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss: 0.5712  Validation Accuracy: 59.80%\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss: 0.5462  Validation Accuracy: 59.96%\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss: 0.5092  Validation Accuracy: 60.88%\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss: 0.5524  Validation Accuracy: 59.22%\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss: 0.5847  Validation Accuracy: 57.94%\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss: 0.6094  Validation Accuracy: 58.72%\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss: 0.5179  Validation Accuracy: 60.36%\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss: 0.5194  Validation Accuracy: 60.72%\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss: 0.5443  Validation Accuracy: 59.20%\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss: 0.5528  Validation Accuracy: 58.94%\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss: 0.5715  Validation Accuracy: 59.70%\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss: 0.5367  Validation Accuracy: 60.66%\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss: 0.5097  Validation Accuracy: 60.80%\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss: 0.5304  Validation Accuracy: 59.48%\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss: 0.5281  Validation Accuracy: 59.82%\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss: 0.5760  Validation Accuracy: 59.48%\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss: 0.5771  Validation Accuracy: 59.98%\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss: 0.5215  Validation Accuracy: 60.66%\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss: 0.5347  Validation Accuracy: 59.44%\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss: 0.5376  Validation Accuracy: 60.06%\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss: 0.5589  Validation Accuracy: 59.72%\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss: 0.5706  Validation Accuracy: 60.38%\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss: 0.4994  Validation Accuracy: 60.96%\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss: 0.5322  Validation Accuracy: 59.60%\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss: 0.5271  Validation Accuracy: 59.76%\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss: 0.5807  Validation Accuracy: 60.44%\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss: 0.5734  Validation Accuracy: 60.24%\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss: 0.5006  Validation Accuracy: 60.60%\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss: 0.5466  Validation Accuracy: 59.06%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.5889947354793549\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvnu3bCoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d49297400>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
